---
title: "BCB420_A1"
output: html_notebook
---
# Preperation and choosing a dataset.

We will start by checking if the required libraries and files are installed.
```{r}
if (!requireNamespace("BiocManager", quietly = TRUE)){
  install.packages("BiocManager")
}
if (!requireNamespace("GEOmetadb", quietly = TRUE)){
  BiocManager::install("GEOmetadb")
}
if (!file.exists('GEOmetadb.sqlite')){
  getSQLiteFile()
}
if (!requireNamespace("edgeR", quietly = TRUE)){
  BiocManager::install("edgeR")
}
library(BiocManager)
library(edgeR)
library(GEOmetadb)
library(biomaRt)
library(knitr)
```

Then we will use the following commands to query the GEOmetadb (GEO database) and look for a dataset with  a "raw counts" txt supplentary file.

```{r}
sql <- paste("SELECT DISTINCT gse.title,gse.gse, gpl.title,",
             " gse.submission_date,",
             " gse.supplementary_file",
             "FROM",
             "  gse JOIN gse_gpl ON gse_gpl.gse=gse.gse",
             "  JOIN gpl ON gse_gpl.gpl=gpl.gpl",
             "WHERE",
             "  gse.submission_date > '2015-01-01' AND",
             "  gse.title LIKE '%cancer%' AND", 
             "  gpl.organism LIKE '%Homo sapiens%' AND",
             "  gpl.title LIKE '%HiSeq%' ",
             "  ORDER BY gse.submission_date DESC",sep=" ")

con <- dbConnect(SQLite(), 'GEOmetadb.sqlite')
rs <- dbGetQuery(con, sql)
unlist(lapply(rs$supplementary_file,
              FUN = function(x){x <- unlist(strsplit(x,";")) ;
              x <- x[grep(x,pattern="txt",ignore.case = TRUE)];
              tail(unlist(strsplit(x,"/")),n=1)})) [1:10]
```


The dataset I chose was 'GSE106169' which is related to the polyol pathway link with glucose metalism of cancer cells. Download the counts dataset and create a dataframe with the data.

```{r}
sfiles = getGEOSuppFiles('GSE106169')
fnames = rownames(sfiles)
data = read.delim(fnames[1], header = TRUE, check.names = FALSE)
dim(data)
head(data)
```

# Exploring the data.

### Duplicate Genes.
We will first see check for duplicate genes in our data. As we see below, we have no duplicate genes in our data, thus no action is required for duplicate genes. 
```{r}
summarized_gene_counts <- sort(table(data$Geneid),decreasing = TRUE)
kable(summarized_gene_counts[which(summarized_gene_counts > 1)[1:10]], format = 'html')
```

### Grouping the data.
We will define groups for the data for later (normalization process).
```{r}
samples <- data.frame(lapply(colnames(data)[2:10], 
                             FUN=function(x){unlist(strsplit(x, split = " "))[c(1)]}))
colnames(samples) <- colnames(data)[2:10]
rownames(samples) <- c("condition")
samples <- data.frame(t(samples))
samples
```

# Filtering the data.
We will filter out low counts from the data according to the edgeR protocol. The threshold is set to 3 because edgeR recommends the threshold to be the number of replications which in our dataset is 3 (Would be interesting to see what happens if we change our threshold, but we will keep it to 3 for now).
```{r}
cpms = edgeR::cpm(data[, 2:10])
rownames(cpms) <- data[, 1]
keep = rowSums(cpms > 1) >= 3
dataFiltered = data[keep, ]
head(dataFiltered)
```
```{r}
dim(data)
dim(dataFiltered)
```


We can see that the total number of genes reduced to 14040 from 57905. Thus, we can say we have succesfully filtered low expression/noninformative data.

# Mapping the data.
We will map the filtered data to HUGO gene symbols using grch37 biomart.
```{r}
ensembl <- useMart(biomart = "ensembl", dataset = "hsapiens_gene_ensembl", host="grch37.ensembl.org")

conversionStash <- "data_conversion.rds"
if(file.exists(conversionStash)){
  dataIdConversion <- readRDS(conversionStash)
} else {
  dataIdConversion <- getBM(attributes = c("ensembl_gene_id", "hgnc_symbol"),
                               filters = c("ensembl_gene_id"),
                               values = dataFiltered$Geneid,
                               mart = ensembl)
  saveRDS(dataIdConversion, conversionStash)
}

dataIdConversion
```

# Normalizing data.

Before, we deal with missing identifiers from the mapping. We will normalize our data first using the TMM method.
```{r}
filteredDataMatrix <- as.matrix(dataFiltered[,2:10])
rownames(filteredDataMatrix) <- dataFiltered$Geneid
d = DGEList(counts = filteredDataMatrix, group = samples$condition)
d = calcNormFactors(d)
normalizedCounts <- cpm(d)
```

Then we will merge the normalizedCounts with our identifiers that we mapped in our previous step.
```{r}
dataFilteredAnnot <- merge(dataIdConversion, normalizedCounts, by.x = 1, by.y = 0, all.y=TRUE)
kable(dataFilteredAnnot[1:5,1:10],type = "html")
```


## Pre-Normalization vs post-normalization density plots.

### Pre-normalization
```{r}
pre_data2plot <- log2(cpm(dataFiltered[, 2:10]))
pre_counts_density <- apply(log2(cpm(dataFiltered[, 2:10])), 2, density)
      xlim <- 0; ylim <- 0
      for (i in 1:length(pre_counts_density)) {
        xlim <- range(c(xlim, pre_counts_density[[i]]$x)); 
        ylim <- range(c(ylim, pre_counts_density[[i]]$y))
      }
      cols <- rainbow(length(pre_counts_density))
      ltys <- rep(1, length(pre_counts_density))
      #plot the first density plot to initialize the plot
      plot(pre_counts_density[[1]], xlim=xlim, ylim=ylim, type="n", 
           ylab="Smoothing density of log2-CPM", main="", cex.lab = 0.85)
      #plot each line
      for (i in 1:length(pre_counts_density)) lines(pre_counts_density[[i]], col=cols[i], lty=ltys[i])
      #create legend
      legend("topright", colnames(pre_data2plot),  
             col=cols, lty=ltys, cex=0.75, 
             border ="blue",  text.col = "green4", 
             merge = TRUE, bg = "gray90")

```

### Post-Normalization
```{r}
post_data2plot <- log2(cpm(dataFilteredAnnot[, 3:11]))
post_counts_density <- apply(log2(cpm(dataFilteredAnnot[, 3:11])), 2, density)
      xlim <- 0; ylim <- 0
      for (i in 1:length(post_counts_density)) {
        xlim <- range(c(xlim, post_counts_density[[i]]$x)); 
        ylim <- range(c(ylim, post_counts_density[[i]]$y))
      }
      cols <- rainbow(length(post_counts_density))
      ltys <- rep(1, length(post_counts_density))
      #plot the first density plot to initialize the plot
      plot(post_counts_density[[1]], xlim=xlim, ylim=ylim, type="n", 
           ylab="Smoothing density of log2-CPM", main="", cex.lab = 0.85)
      #plot each line
      for (i in 1:length(post_counts_density)) lines(post_counts_density[[i]], col=cols[i], lty=ltys[i])
      #create legend
      legend("topright", colnames(post_data2plot),  
             col=cols, lty=ltys, cex=0.75, 
             border ="blue",  text.col = "green4", 
             merge = TRUE, bg = "gray90")
```

No significance difference between pre and post normalization. I belive this is due to the fact that my origninal data happens to be close to a normal distribution, thus normalization with TMM method did not make any significant changes.


# Missing identifiers.
We will first check how many missing identifiers are present in our data right now.

```{r}
ensembl_id_missing_gene <- dataFilteredAnnot$ensembl_gene_id[which((dataFilteredAnnot$hgnc_symbol) == "")]
length(ensembl_id_missing_gene)
```

We have 302 missing identifiers. Lets see some of the missing identifiers rows.
```{r}
kable(dataFilteredAnnot[which((dataFilteredAnnot$hgnc_symbol == ""))[1:5],1:5], type="html")
```

We will try rematching the missing genes using "external gene names" which gives the gene name for those that are missing a symbol. (I chose this method as it seems to be the best way to represent my data without needing to delete any data which I think is risky. Any feedback or other possible ways would be greatly appreciated). Later on, if we decide to chuck out the genes that were not mapped property with "hgnc symbols), we can always use the missDataIdConversion file to check these and chuck it out. However, for this assignment I decided to use "external gene names" for the unmapped genes. 

```{r}
ensembl <- useMart(biomart = "ensembl", dataset = "hsapiens_gene_ensembl", host="grch37.ensembl.org")
conversionStash <- "missing_data_conversion.rds"
if(file.exists(conversionStash)){
  missDataIdConversion <- readRDS(conversionStash)
} else {
  missDataIdConversion <- getBM(attributes = c("ensembl_gene_id", "external_gene_name"),
                               filters = c("ensembl_gene_id"),
                               values = ensembl_id_missing_gene,
                               mart = ensembl)
  saveRDS(missDataIdConversion, conversionStash)
}

for (i in 1:length(missDataIdConversion$ensembl_gene_id)){
  index <- match(missDataIdConversion$ensembl_gene_id[i], dataFilteredAnnot$ensembl_gene_id)
  dataFilteredAnnot[index, 1:10]$hgnc_symbol = missDataIdConversion$external_gene_name[i]
}

#Check again for missing symbols
ensembl_id_missing_gene <- dataFilteredAnnot$ensembl_gene_id[which(dataFilteredAnnot$hgnc_symbol == "")]
length(ensembl_id_missing_gene)
```

Now we have hgnc_symbols for all genes (some are gene names for genes that do not have hgncn_symbols available). 

# Final Data Set.

```{r}
head(dataFilteredAnnot)
dim(dataFilteredAnnot)
```

# Interpertation.

### What are the control and test conditions of the dataset?
The experiment consists of 3 replications of control condition and 2 test conditions each replicated 3 times. The first test condition was AKR1B1 knockdown and the second test condition was targetting the SORD enyzme. 

### Why is the dataset of interest to you?
First, I was looking for a dataset related to leaukemia with a good/clean raw counts files because I worked with leukemia RNA-seq datasets during the summer for a research project. However, I couldn't find a dataset that I liked, so I decided to use "GSE106169" which is regarding lung cancer. 

### Were there expression values that were not unique for specific genes? How did you handle these?
There were 3 expression values that were not unique for specific genes as you see below. I have kept them for now, however, I can always remove them if neccessary as we have the data below. I left them because as long as we have the data, we can always adjust in the future if any problems arise and removing them so quickly right now does not seem safe to do. 
```{r}
n_occur <- data.frame(table(dataFilteredAnnot$hgnc_symbol))
n_occur[n_occur$Freq > 1, ]
```

### Were there expression values that could not be mapped to current HUGO symbols?
Yes, there were expression values that could not be mapped to current HUGO symbols as mentions above. I have used the GRCh 37 biomart to map the expression values as the associated paper stated that they aligned with GRCh 37. However, there were still 1012 genes that were not mapped. I decided to map them using gene names instead of symbols using the "external_gene_name" attribute This was the best solution I could think of for mapping all by expression values without deleting or leavin them N/A. There is will data regarding the missed map expression values (missing_data_conversion.rds), thus we can remove them or use other methods to map in the future. The not mapped genes are about ~7.7% of the dataset.

The above was my interpertation of the unmapped values, would there be a better way to handle this case?

### How many outliers were removed?
43865 outliers were removed as you may have seen in the above section 'Filtering data'.

### How did you handle replicates?
Experimental replication in this case, each test condition and control was replicated 3 times were handled by assigning groups. As you seen in the "grouping the data" section, I have grouped the replication into 3 categories (named after their test condition / control condition).
As for gene dupicates, there were no duplications present in my data thus I did not have to handle these cases.

### What is the final coverage of your dataset?
The final coverage of my data set is 14042 expression values across 9 samples (3 replications each in each conditions).


